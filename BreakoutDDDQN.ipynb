{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xv719uBL9x8t",
        "outputId": "ced9da04-f8c3-40e2-eee0-c4ac1a94ce3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.16-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.11-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 58.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=403b88cd44e8eaeacea20d46973d7ee210ed362dacd444f565e584eaacf0717e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.11 setproctitle-1.2.3 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.16\n"
          ]
        }
      ],
      "source": [
        "%pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bP4uW4_6BYy",
        "outputId": "cc72d641-39aa-4163-d6f1-52254bc6badf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.23.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.6)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.11.3)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.4\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.4->gym[accept-rom-license,atari]) (5.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym[accept-rom-license,atari]) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym[accept-rom-license,atari]) (4.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=2da2a60b82a37c2756e0451de815e26a5bddbd04f194bf1f9d81b509358d0bf9\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-D98CddQuwKG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "import gym\n",
        "import cv2\n",
        "import time\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "PtS7Z6p996ek",
        "outputId": "68ac0e7b-cbc7-4b1d-d290-2499b523e4a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tornado/httputil.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  class HTTPHeaders(collections.MutableMapping):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220506_145055-yw0y4vx1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/adundee/Breakout-Pytorch/runs/yw0y4vx1\" target=\"_blank\">bright-blaze-16</a></strong> to <a href=\"https://wandb.ai/adundee/Breakout-Pytorch\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/adundee/Breakout-Pytorch/runs/yw0y4vx1?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fe9b7eb5c10>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "wandb.init(\n",
        "  project=\"Breakout-Pytorch\",\n",
        "  tags=[\"DQN\", \"CNN\", \"RL\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I8rGMlN2uzgk"
      },
      "outputs": [],
      "source": [
        "GPU = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HxF5-bzUu1q-"
      },
      "outputs": [],
      "source": [
        "class DDQN(nn.Module):\n",
        "    def __init__(self, h, w, output_size):\n",
        "        super(DDQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=4,  out_channels=32, kernel_size=8, stride=4)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        \n",
        "        convw, convh = self.conv2d_size_calc(w, h, kernel_size=8, stride=4)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=4, stride=2)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=3, stride=1)\n",
        "        linear_input_size = convw * convh * 64  # Last conv layer's out sizes\n",
        "\n",
        "        # Action layer\n",
        "        self.Alinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
        "        self.Alrelu = nn.LeakyReLU()  # Linear 1 activation funct\n",
        "        self.Alinear2 = nn.Linear(in_features=128, out_features=output_size)\n",
        "\n",
        "        # State Value layer\n",
        "        self.Vlinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
        "        self.Vlrelu = nn.LeakyReLU()  # Linear 1 activation funct\n",
        "        self.Vlinear2 = nn.Linear(in_features=128, out_features=1)  # Only 1 node\n",
        "\n",
        "    def init_weights(self, m):\n",
        "        if type(m) == nn.Linear:\n",
        "            torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "            #m.bias.data.fill_(0.0)\n",
        "        \n",
        "        if type(m) == nn.Conv2d:\n",
        "            torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "            m.bias.data.fill_(0.1)\n",
        "\n",
        "    def conv2d_size_calc(self, w, h, kernel_size=5, stride=2):\n",
        "        \"\"\"\n",
        "        Calcs conv layers output image sizes\n",
        "        \"\"\"\n",
        "        next_w = (w - (kernel_size - 1) - 1) // stride + 1\n",
        "        next_h = (h - (kernel_size - 1) - 1) // stride + 1\n",
        "        return next_w, next_h\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten every batch\n",
        "\n",
        "        Ax = self.Alrelu(self.Alinear1(x))\n",
        "        Ax = self.Alinear2(Ax)  # No activation on last layer\n",
        "\n",
        "        Vx = self.Vlrelu(self.Vlinear1(x))\n",
        "        Vx = self.Vlinear2(Vx)  # No activation on last layer\n",
        "\n",
        "        q = Vx + (Ax - Ax.mean())\n",
        "\n",
        "        return q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WApt7r71sq8g"
      },
      "outputs": [],
      "source": [
        "#AGENT COMBINE CELL\n",
        "class DDDQNAgent:\n",
        "    def __init__(self, environment):\n",
        "        \"\"\"Set the hyperparameters for our agent\"\"\"\n",
        "\n",
        "        # Set the discount rate\n",
        "        self.gamma = 0.99\n",
        "\n",
        "        # Set our initial exploration parameter epsilon\n",
        "        self.epsilon = 1\n",
        "\n",
        "        # Get number of actions available to the agent\n",
        "        self.n_actions = environment.action_space.n\n",
        "\n",
        "        # Initiate our replay memory\n",
        "        self.replay_buffer = deque(maxlen=50000)\n",
        "\n",
        "        # Create two model for DDQN algorithm\n",
        "        self.Net = DDQN(h=84, w=84, output_size=self.n_actions)\n",
        "        self.main_model = self.Net.to(GPU)\n",
        "        # self.target_model = DDQN(h=self.target_h, w=self.target_w, output_size=self.action_size).to(GPU)\n",
        "        self.target_model=copy.deepcopy(self.main_model).to(GPU)\n",
        "        self.Net.apply(self.Net.init_weights)\n",
        "        self.target_model.load_state_dict(self.main_model.state_dict())\n",
        "        self.target_model.eval()\n",
        "\n",
        "        # Set out optimizer\n",
        "        self.optimizer = optim.Adam(self.main_model.parameters(), lr=0.0003)\n",
        "\n",
        "    def convert_to_grey(self, image):\n",
        "        \"\"\"Convert the image to greyscale\"\"\"\n",
        "        return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    def crop_image(self, image, h_start=34, h_end=194, w_start=0, w_end=160):\n",
        "        \"\"\"Crop the image\"\"\"\n",
        "        return image[h_start:h_end, w_start:w_end]\n",
        "\n",
        "    def resize_reshape(self, image, height=84, width=84):\n",
        "        \"\"\"Resize and reshape our image\"\"\"\n",
        "        image = cv2.resize(image, (width, height))\n",
        "        return image.reshape(width, height)\n",
        "     \n",
        "\n",
        "    def preProcess(self, image):\n",
        "        img = self.convert_to_grey(image)\n",
        "        img = self.crop_image(img)\n",
        "        img = self.resize_reshape(img)\n",
        "\n",
        "        return img / 255\n",
        "\n",
        "    def random_action(self):\n",
        "      return random.randrange(self.n_actions)\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() > self.epsilon:\n",
        "            with torch.no_grad():\n",
        "                  state = torch.tensor(state, dtype=torch.float, device=GPU)\n",
        "                  state = state.unsqueeze(0)\n",
        "                  qs = self.main_model.forward(state)\n",
        "                  action = torch.argmax(qs).item()\n",
        "            return action\n",
        "        else:\n",
        "            action = random.randrange(self.n_actions)\n",
        "            return action\n",
        "\n",
        "    def predict(self, online, state):\n",
        "        if online:\n",
        "          return self.main_model(state)\n",
        "        else:\n",
        "          return self.target_model(state)\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        #Delays learning until agent has sufficient experience.\n",
        "        if len(agent.replay_buffer) < 40000:\n",
        "            loss, max_q = [0, 0]\n",
        "            return loss, max_q\n",
        "\n",
        "        #Take minibatch of size 64 freom repay buffer\n",
        "        random_samples = random.sample(self.replay_buffer, 64) #Change 64 to change size of batch\n",
        "        state, action, reward, next_state, done = zip(*random_samples)\n",
        "\n",
        "\n",
        "        state = np.concatenate(state)\n",
        "        next_state = np.concatenate(next_state)\n",
        "\n",
        "        #Feed replays to Neural nets to observe expected q_values\n",
        "        state = torch.tensor(state, dtype=torch.float, device=GPU)\n",
        "        state_q_values = self.predict(True,state)\n",
        "        state_q_val_max = torch.max(state_q_values).item()\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float, device=GPU)\n",
        "        next_states_q_values = self.predict(True, next_state)\n",
        "        next_states_target_q_values = self.predict(False, next_state)\n",
        "\n",
        "\n",
        "        action = torch.tensor(action, dtype=torch.long, device=GPU)\n",
        "        selected_q_value = state_q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "        next_states_target_q_value = next_states_target_q_values.gather(1, next_states_q_values.max(1)[1].unsqueeze(\n",
        "            1)).squeeze(1)\n",
        "        reward = torch.tensor(reward, dtype=torch.float, device=GPU)\n",
        "        done = torch.tensor(done, dtype=torch.float, device=GPU)\n",
        "\n",
        "        bellQ = reward + self.gamma * next_states_target_q_value * (1 - done)\n",
        "\n",
        "        #Find difference between expected value and action value\n",
        "        difference_q_value = selected_q_value - bellQ.detach()\n",
        "        loss = (difference_q_value**2).mean()\n",
        "\n",
        "        #Use difference to update model weightings and hopefull improve performance\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "        return loss, state_q_val_max\n",
        "\n",
        "    def storeResults(self, state, action, reward, nextState, done):\n",
        "        self.replay_buffer.append([state[None, :], action, reward, nextState[None, :], done])\n",
        "\n",
        "    def updateEpsilon(self):\n",
        "        if self.epsilon > 0.05:\n",
        "            self.epsilon *= 0.99\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ve4vYDe3bozg",
        "outputId": "2d18d585-d2a1-475b-999f-5fa1f16d8da1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Episode = 0,  Episode Reward = 3.0, Rolling Average = 3.0, Current Epsilon = 1, Total Step = 263\n",
            "Current Episode = 1,  Episode Reward = 1.0, Rolling Average = 2.0, Current Epsilon = 1, Total Step = 430\n",
            "Current Episode = 2,  Episode Reward = 0.0, Rolling Average = 1.3333333333333333, Current Epsilon = 1, Total Step = 564\n",
            "Current Episode = 3,  Episode Reward = 0.0, Rolling Average = 1.0, Current Epsilon = 1, Total Step = 702\n",
            "Current Episode = 4,  Episode Reward = 0.0, Rolling Average = 0.8, Current Epsilon = 1, Total Step = 837\n",
            "Current Episode = 5,  Episode Reward = 0.0, Rolling Average = 0.6666666666666666, Current Epsilon = 1, Total Step = 978\n",
            "Current Episode = 6,  Episode Reward = 3.0, Rolling Average = 1.0, Current Epsilon = 0.99, Total Step = 1224\n",
            "Current Episode = 7,  Episode Reward = 3.0, Rolling Average = 1.25, Current Epsilon = 0.99, Total Step = 1489\n",
            "Current Episode = 8,  Episode Reward = 0.0, Rolling Average = 1.1111111111111112, Current Epsilon = 0.99, Total Step = 1617\n",
            "Current Episode = 9,  Episode Reward = 2.0, Rolling Average = 1.2, Current Epsilon = 0.99, Total Step = 1807\n",
            "Current Episode = 10,  Episode Reward = 0.0, Rolling Average = 1.0909090909090908, Current Epsilon = 0.99, Total Step = 1935\n",
            "Current Episode = 11,  Episode Reward = 0.0, Rolling Average = 1.0, Current Epsilon = 0.9801, Total Step = 2081\n",
            "Current Episode = 12,  Episode Reward = 2.0, Rolling Average = 1.0769230769230769, Current Epsilon = 0.9801, Total Step = 2292\n",
            "Current Episode = 13,  Episode Reward = 1.0, Rolling Average = 1.0714285714285714, Current Epsilon = 0.9801, Total Step = 2453\n",
            "Current Episode = 14,  Episode Reward = 0.0, Rolling Average = 1.0, Current Epsilon = 0.9801, Total Step = 2594\n",
            "Current Episode = 15,  Episode Reward = 1.0, Rolling Average = 1.0, Current Epsilon = 0.9801, Total Step = 2782\n",
            "Current Episode = 16,  Episode Reward = 0.0, Rolling Average = 0.9411764705882353, Current Epsilon = 0.9801, Total Step = 2908\n",
            "Current Episode = 17,  Episode Reward = 2.0, Rolling Average = 1.0, Current Epsilon = 0.9702989999999999, Total Step = 3133\n",
            "Current Episode = 18,  Episode Reward = 0.0, Rolling Average = 0.9473684210526315, Current Epsilon = 0.9702989999999999, Total Step = 3267\n",
            "Current Episode = 19,  Episode Reward = 2.0, Rolling Average = 1.0, Current Epsilon = 0.9702989999999999, Total Step = 3479\n",
            "Current Episode = 20,  Episode Reward = 1.0, Rolling Average = 1.0, Current Epsilon = 0.9702989999999999, Total Step = 3640\n",
            "Current Episode = 21,  Episode Reward = 3.0, Rolling Average = 1.0909090909090908, Current Epsilon = 0.9702989999999999, Total Step = 3910\n",
            "Current Episode = 22,  Episode Reward = 1.0, Rolling Average = 1.0869565217391304, Current Epsilon = 0.96059601, Total Step = 4084\n",
            "Current Episode = 23,  Episode Reward = 0.0, Rolling Average = 1.0416666666666667, Current Epsilon = 0.96059601, Total Step = 4221\n",
            "Current Episode = 24,  Episode Reward = 1.0, Rolling Average = 1.04, Current Epsilon = 0.96059601, Total Step = 4405\n",
            "Current Episode = 25,  Episode Reward = 0.0, Rolling Average = 1.0, Current Epsilon = 0.96059601, Total Step = 4532\n",
            "Current Episode = 26,  Episode Reward = 2.0, Rolling Average = 1.037037037037037, Current Epsilon = 0.96059601, Total Step = 4777\n",
            "Current Episode = 27,  Episode Reward = 3.0, Rolling Average = 1.1071428571428572, Current Epsilon = 0.9509900498999999, Total Step = 5019\n",
            "Current Episode = 28,  Episode Reward = 1.0, Rolling Average = 1.103448275862069, Current Epsilon = 0.9509900498999999, Total Step = 5195\n",
            "Current Episode = 29,  Episode Reward = 2.0, Rolling Average = 1.1333333333333333, Current Epsilon = 0.9509900498999999, Total Step = 5411\n",
            "Current Episode = 30,  Episode Reward = 1.0, Rolling Average = 1.1290322580645162, Current Epsilon = 0.9509900498999999, Total Step = 5581\n",
            "Current Episode = 31,  Episode Reward = 1.0, Rolling Average = 1.125, Current Epsilon = 0.9509900498999999, Total Step = 5753\n",
            "Current Episode = 32,  Episode Reward = 0.0, Rolling Average = 1.0909090909090908, Current Epsilon = 0.9509900498999999, Total Step = 5877\n",
            "Current Episode = 33,  Episode Reward = 0.0, Rolling Average = 1.0588235294117647, Current Epsilon = 0.9414801494009999, Total Step = 6025\n",
            "Current Episode = 34,  Episode Reward = 1.0, Rolling Average = 1.0571428571428572, Current Epsilon = 0.9414801494009999, Total Step = 6209\n",
            "Current Episode = 35,  Episode Reward = 1.0, Rolling Average = 1.0555555555555556, Current Epsilon = 0.9414801494009999, Total Step = 6412\n",
            "Current Episode = 36,  Episode Reward = 0.0, Rolling Average = 1.027027027027027, Current Epsilon = 0.9414801494009999, Total Step = 6575\n",
            "Current Episode = 37,  Episode Reward = 2.0, Rolling Average = 1.0526315789473684, Current Epsilon = 0.9414801494009999, Total Step = 6785\n",
            "Current Episode = 38,  Episode Reward = 2.0, Rolling Average = 1.0769230769230769, Current Epsilon = 0.9320653479069899, Total Step = 7000\n",
            "Current Episode = 39,  Episode Reward = 4.0, Rolling Average = 1.15, Current Epsilon = 0.9320653479069899, Total Step = 7271\n",
            "Current Episode = 40,  Episode Reward = 3.0, Rolling Average = 1.1951219512195121, Current Epsilon = 0.9320653479069899, Total Step = 7552\n",
            "Current Episode = 41,  Episode Reward = 2.0, Rolling Average = 1.2142857142857142, Current Epsilon = 0.9320653479069899, Total Step = 7750\n",
            "Current Episode = 42,  Episode Reward = 0.0, Rolling Average = 1.186046511627907, Current Epsilon = 0.9320653479069899, Total Step = 7885\n",
            "Current Episode = 43,  Episode Reward = 0.0, Rolling Average = 1.1590909090909092, Current Epsilon = 0.92274469442792, Total Step = 8028\n",
            "Current Episode = 44,  Episode Reward = 0.0, Rolling Average = 1.1333333333333333, Current Epsilon = 0.92274469442792, Total Step = 8162\n",
            "Current Episode = 45,  Episode Reward = 3.0, Rolling Average = 1.173913043478261, Current Epsilon = 0.92274469442792, Total Step = 8444\n",
            "Current Episode = 46,  Episode Reward = 3.0, Rolling Average = 1.2127659574468086, Current Epsilon = 0.92274469442792, Total Step = 8689\n",
            "Current Episode = 47,  Episode Reward = 2.0, Rolling Average = 1.2291666666666667, Current Epsilon = 0.92274469442792, Total Step = 8889\n",
            "Current Episode = 48,  Episode Reward = 0.0, Rolling Average = 1.2040816326530612, Current Epsilon = 0.9135172474836407, Total Step = 9025\n",
            "Current Episode = 49,  Episode Reward = 3.0, Rolling Average = 1.24, Current Epsilon = 0.9135172474836407, Total Step = 9326\n",
            "Current Episode = 50,  Episode Reward = 0.0, Rolling Average = 1.2156862745098038, Current Epsilon = 0.9135172474836407, Total Step = 9455\n",
            "Current Episode = 51,  Episode Reward = 3.0, Rolling Average = 1.25, Current Epsilon = 0.9135172474836407, Total Step = 9693\n",
            "Current Episode = 52,  Episode Reward = 2.0, Rolling Average = 1.2641509433962264, Current Epsilon = 0.9135172474836407, Total Step = 9901\n",
            "Current Episode = 53,  Episode Reward = 4.0, Rolling Average = 1.3148148148148149, Current Epsilon = 0.9043820750088043, Total Step = 10186\n",
            "Current Episode = 54,  Episode Reward = 0.0, Rolling Average = 1.290909090909091, Current Epsilon = 0.9043820750088043, Total Step = 10319\n",
            "Current Episode = 55,  Episode Reward = 2.0, Rolling Average = 1.3035714285714286, Current Epsilon = 0.9043820750088043, Total Step = 10521\n",
            "Current Episode = 56,  Episode Reward = 0.0, Rolling Average = 1.280701754385965, Current Epsilon = 0.9043820750088043, Total Step = 10656\n",
            "Current Episode = 57,  Episode Reward = 3.0, Rolling Average = 1.3103448275862069, Current Epsilon = 0.9043820750088043, Total Step = 10903\n",
            "Current Episode = 58,  Episode Reward = 3.0, Rolling Average = 1.3389830508474576, Current Epsilon = 0.8953382542587163, Total Step = 11153\n",
            "Current Episode = 59,  Episode Reward = 1.0, Rolling Average = 1.3333333333333333, Current Epsilon = 0.8953382542587163, Total Step = 11310\n",
            "Current Episode = 60,  Episode Reward = 1.0, Rolling Average = 1.3278688524590163, Current Epsilon = 0.8953382542587163, Total Step = 11468\n",
            "Current Episode = 61,  Episode Reward = 1.0, Rolling Average = 1.3225806451612903, Current Epsilon = 0.8953382542587163, Total Step = 11629\n",
            "Current Episode = 62,  Episode Reward = 0.0, Rolling Average = 1.3015873015873016, Current Epsilon = 0.8953382542587163, Total Step = 11765\n",
            "Current Episode = 63,  Episode Reward = 1.0, Rolling Average = 1.296875, Current Epsilon = 0.8953382542587163, Total Step = 11945\n",
            "Current Episode = 64,  Episode Reward = 3.0, Rolling Average = 1.323076923076923, Current Epsilon = 0.8863848717161291, Total Step = 12222\n",
            "Current Episode = 65,  Episode Reward = 0.0, Rolling Average = 1.303030303030303, Current Epsilon = 0.8863848717161291, Total Step = 12364\n",
            "Current Episode = 66,  Episode Reward = 2.0, Rolling Average = 1.3134328358208955, Current Epsilon = 0.8863848717161291, Total Step = 12562\n",
            "Current Episode = 67,  Episode Reward = 3.0, Rolling Average = 1.338235294117647, Current Epsilon = 0.8863848717161291, Total Step = 12795\n",
            "Current Episode = 68,  Episode Reward = 2.0, Rolling Average = 1.3478260869565217, Current Epsilon = 0.8775210229989678, Total Step = 13005\n",
            "Current Episode = 69,  Episode Reward = 1.0, Rolling Average = 1.3428571428571427, Current Epsilon = 0.8775210229989678, Total Step = 13186\n",
            "Current Episode = 70,  Episode Reward = 3.0, Rolling Average = 1.3661971830985915, Current Epsilon = 0.8775210229989678, Total Step = 13439\n",
            "Current Episode = 71,  Episode Reward = 5.0, Rolling Average = 1.4166666666666667, Current Epsilon = 0.8775210229989678, Total Step = 13756\n",
            "Current Episode = 72,  Episode Reward = 1.0, Rolling Average = 1.4109589041095891, Current Epsilon = 0.8775210229989678, Total Step = 13912\n",
            "Current Episode = 73,  Episode Reward = 2.0, Rolling Average = 1.4189189189189189, Current Epsilon = 0.8687458127689781, Total Step = 14114\n",
            "Current Episode = 74,  Episode Reward = 2.0, Rolling Average = 1.4266666666666667, Current Epsilon = 0.8687458127689781, Total Step = 14335\n",
            "Current Episode = 75,  Episode Reward = 3.0, Rolling Average = 1.4473684210526316, Current Epsilon = 0.8687458127689781, Total Step = 14564\n",
            "Current Episode = 76,  Episode Reward = 2.0, Rolling Average = 1.4545454545454546, Current Epsilon = 0.8687458127689781, Total Step = 14756\n",
            "Current Episode = 77,  Episode Reward = 1.0, Rolling Average = 1.4487179487179487, Current Epsilon = 0.8687458127689781, Total Step = 14917\n",
            "Current Episode = 78,  Episode Reward = 0.0, Rolling Average = 1.4303797468354431, Current Epsilon = 0.8600583546412883, Total Step = 15051\n",
            "Current Episode = 79,  Episode Reward = 2.0, Rolling Average = 1.4375, Current Epsilon = 0.8600583546412883, Total Step = 15246\n",
            "Current Episode = 80,  Episode Reward = 3.0, Rolling Average = 1.4567901234567902, Current Epsilon = 0.8600583546412883, Total Step = 15500\n",
            "Current Episode = 81,  Episode Reward = 2.0, Rolling Average = 1.4634146341463414, Current Epsilon = 0.8600583546412883, Total Step = 15719\n",
            "Current Episode = 82,  Episode Reward = 0.0, Rolling Average = 1.4457831325301205, Current Epsilon = 0.8600583546412883, Total Step = 15874\n",
            "Current Episode = 83,  Episode Reward = 2.0, Rolling Average = 1.4523809523809523, Current Epsilon = 0.8514577710948754, Total Step = 16068\n",
            "Current Episode = 84,  Episode Reward = 3.0, Rolling Average = 1.4705882352941178, Current Epsilon = 0.8514577710948754, Total Step = 16296\n",
            "Current Episode = 85,  Episode Reward = 0.0, Rolling Average = 1.4534883720930232, Current Epsilon = 0.8514577710948754, Total Step = 16427\n",
            "Current Episode = 86,  Episode Reward = 3.0, Rolling Average = 1.471264367816092, Current Epsilon = 0.8514577710948754, Total Step = 16688\n",
            "Current Episode = 87,  Episode Reward = 1.0, Rolling Average = 1.4659090909090908, Current Epsilon = 0.8514577710948754, Total Step = 16839\n",
            "Current Episode = 88,  Episode Reward = 1.0, Rolling Average = 1.4606741573033708, Current Epsilon = 0.8429431933839266, Total Step = 17008\n",
            "Current Episode = 89,  Episode Reward = 4.0, Rolling Average = 1.488888888888889, Current Epsilon = 0.8429431933839266, Total Step = 17295\n",
            "Current Episode = 90,  Episode Reward = 1.0, Rolling Average = 1.4835164835164836, Current Epsilon = 0.8429431933839266, Total Step = 17456\n",
            "Current Episode = 91,  Episode Reward = 3.0, Rolling Average = 1.5, Current Epsilon = 0.8429431933839266, Total Step = 17688\n",
            "Current Episode = 92,  Episode Reward = 2.0, Rolling Average = 1.5053763440860215, Current Epsilon = 0.8429431933839266, Total Step = 17896\n",
            "Current Episode = 93,  Episode Reward = 3.0, Rolling Average = 1.5212765957446808, Current Epsilon = 0.8345137614500874, Total Step = 18140\n",
            "Current Episode = 94,  Episode Reward = 2.0, Rolling Average = 1.5263157894736843, Current Epsilon = 0.8345137614500874, Total Step = 18341\n",
            "Current Episode = 95,  Episode Reward = 5.0, Rolling Average = 1.5625, Current Epsilon = 0.8345137614500874, Total Step = 18681\n",
            "Current Episode = 96,  Episode Reward = 3.0, Rolling Average = 1.577319587628866, Current Epsilon = 0.8345137614500874, Total Step = 18914\n",
            "Current Episode = 97,  Episode Reward = 2.0, Rolling Average = 1.5816326530612246, Current Epsilon = 0.8261686238355865, Total Step = 19105\n",
            "Current Episode = 98,  Episode Reward = 2.0, Rolling Average = 1.5858585858585859, Current Epsilon = 0.8261686238355865, Total Step = 19316\n",
            "Current Episode = 99,  Episode Reward = 2.0, Rolling Average = 1.59, Current Epsilon = 0.8261686238355865, Total Step = 19520\n",
            "Current Episode = 100,  Episode Reward = 2.0, Rolling Average = 1.58, Current Epsilon = 0.8261686238355865, Total Step = 19730\n",
            "Current Episode = 101,  Episode Reward = 2.0, Rolling Average = 1.59, Current Epsilon = 0.8261686238355865, Total Step = 19923\n",
            "Current Episode = 102,  Episode Reward = 2.0, Rolling Average = 1.61, Current Epsilon = 0.8179069375972307, Total Step = 20120\n",
            "Current Episode = 103,  Episode Reward = 5.0, Rolling Average = 1.66, Current Epsilon = 0.8179069375972307, Total Step = 20413\n",
            "Current Episode = 104,  Episode Reward = 4.0, Rolling Average = 1.7, Current Epsilon = 0.8179069375972307, Total Step = 20683\n",
            "Current Episode = 105,  Episode Reward = 4.0, Rolling Average = 1.74, Current Epsilon = 0.8179069375972307, Total Step = 20981\n",
            "Current Episode = 106,  Episode Reward = 1.0, Rolling Average = 1.72, Current Epsilon = 0.8097278682212583, Total Step = 21155\n",
            "Current Episode = 107,  Episode Reward = 2.0, Rolling Average = 1.71, Current Epsilon = 0.8097278682212583, Total Step = 21359\n",
            "Current Episode = 108,  Episode Reward = 3.0, Rolling Average = 1.74, Current Epsilon = 0.8097278682212583, Total Step = 21577\n",
            "Current Episode = 109,  Episode Reward = 2.0, Rolling Average = 1.74, Current Epsilon = 0.8097278682212583, Total Step = 21777\n",
            "Current Episode = 110,  Episode Reward = 3.0, Rolling Average = 1.77, Current Epsilon = 0.8016305895390458, Total Step = 22023\n",
            "Current Episode = 111,  Episode Reward = 2.0, Rolling Average = 1.79, Current Epsilon = 0.8016305895390458, Total Step = 22215\n",
            "Current Episode = 112,  Episode Reward = 2.0, Rolling Average = 1.79, Current Epsilon = 0.8016305895390458, Total Step = 22404\n",
            "Current Episode = 113,  Episode Reward = 7.0, Rolling Average = 1.85, Current Epsilon = 0.8016305895390458, Total Step = 22822\n",
            "Current Episode = 114,  Episode Reward = 2.0, Rolling Average = 1.87, Current Epsilon = 0.7936142836436553, Total Step = 23007\n",
            "Current Episode = 115,  Episode Reward = 2.0, Rolling Average = 1.88, Current Epsilon = 0.7936142836436553, Total Step = 23197\n",
            "Current Episode = 116,  Episode Reward = 2.0, Rolling Average = 1.9, Current Epsilon = 0.7936142836436553, Total Step = 23399\n",
            "Current Episode = 117,  Episode Reward = 2.0, Rolling Average = 1.9, Current Epsilon = 0.7936142836436553, Total Step = 23621\n",
            "Current Episode = 118,  Episode Reward = 3.0, Rolling Average = 1.93, Current Epsilon = 0.7936142836436553, Total Step = 23852\n",
            "Current Episode = 119,  Episode Reward = 1.0, Rolling Average = 1.92, Current Epsilon = 0.7856781408072188, Total Step = 24021\n",
            "Current Episode = 120,  Episode Reward = 2.0, Rolling Average = 1.93, Current Epsilon = 0.7856781408072188, Total Step = 24213\n",
            "Current Episode = 121,  Episode Reward = 3.0, Rolling Average = 1.93, Current Epsilon = 0.7856781408072188, Total Step = 24429\n",
            "Current Episode = 122,  Episode Reward = 1.0, Rolling Average = 1.93, Current Epsilon = 0.7856781408072188, Total Step = 24584\n",
            "Current Episode = 123,  Episode Reward = 5.0, Rolling Average = 1.98, Current Epsilon = 0.7856781408072188, Total Step = 24890\n",
            "Current Episode = 124,  Episode Reward = 5.0, Rolling Average = 2.02, Current Epsilon = 0.7778213593991465, Total Step = 25190\n",
            "Current Episode = 125,  Episode Reward = 2.0, Rolling Average = 2.04, Current Epsilon = 0.7778213593991465, Total Step = 25387\n",
            "Current Episode = 126,  Episode Reward = 2.0, Rolling Average = 2.04, Current Epsilon = 0.7778213593991465, Total Step = 25584\n",
            "Current Episode = 127,  Episode Reward = 4.0, Rolling Average = 2.05, Current Epsilon = 0.7778213593991465, Total Step = 25852\n",
            "Current Episode = 128,  Episode Reward = 2.0, Rolling Average = 2.06, Current Epsilon = 0.7700431458051551, Total Step = 26052\n",
            "Current Episode = 129,  Episode Reward = 2.0, Rolling Average = 2.06, Current Epsilon = 0.7700431458051551, Total Step = 26259\n",
            "Current Episode = 130,  Episode Reward = 3.0, Rolling Average = 2.08, Current Epsilon = 0.7700431458051551, Total Step = 26501\n",
            "Current Episode = 131,  Episode Reward = 3.0, Rolling Average = 2.1, Current Epsilon = 0.7700431458051551, Total Step = 26748\n",
            "Current Episode = 132,  Episode Reward = 4.0, Rolling Average = 2.14, Current Epsilon = 0.7623427143471035, Total Step = 27050\n",
            "Current Episode = 133,  Episode Reward = 2.0, Rolling Average = 2.16, Current Epsilon = 0.7623427143471035, Total Step = 27281\n",
            "Current Episode = 134,  Episode Reward = 4.0, Rolling Average = 2.19, Current Epsilon = 0.7623427143471035, Total Step = 27581\n",
            "Current Episode = 135,  Episode Reward = 3.0, Rolling Average = 2.21, Current Epsilon = 0.7623427143471035, Total Step = 27807\n",
            "Current Episode = 136,  Episode Reward = 2.0, Rolling Average = 2.23, Current Epsilon = 0.7547192872036325, Total Step = 28001\n",
            "Current Episode = 137,  Episode Reward = 3.0, Rolling Average = 2.24, Current Epsilon = 0.7547192872036325, Total Step = 28232\n",
            "Current Episode = 138,  Episode Reward = 7.0, Rolling Average = 2.29, Current Epsilon = 0.7547192872036325, Total Step = 28656\n",
            "Current Episode = 139,  Episode Reward = 3.0, Rolling Average = 2.28, Current Epsilon = 0.7547192872036325, Total Step = 28894\n",
            "Current Episode = 140,  Episode Reward = 5.0, Rolling Average = 2.3, Current Epsilon = 0.7471720943315961, Total Step = 29191\n",
            "Current Episode = 141,  Episode Reward = 2.0, Rolling Average = 2.3, Current Epsilon = 0.7471720943315961, Total Step = 29406\n",
            "Current Episode = 142,  Episode Reward = 5.0, Rolling Average = 2.35, Current Epsilon = 0.7471720943315961, Total Step = 29746\n",
            "Current Episode = 143,  Episode Reward = 3.0, Rolling Average = 2.38, Current Epsilon = 0.7471720943315961, Total Step = 29982\n",
            "Current Episode = 144,  Episode Reward = 2.0, Rolling Average = 2.4, Current Epsilon = 0.7397003733882802, Total Step = 30172\n",
            "Current Episode = 145,  Episode Reward = 4.0, Rolling Average = 2.41, Current Epsilon = 0.7397003733882802, Total Step = 30449\n",
            "Current Episode = 146,  Episode Reward = 5.0, Rolling Average = 2.43, Current Epsilon = 0.7397003733882802, Total Step = 30740\n",
            "Current Episode = 147,  Episode Reward = 2.0, Rolling Average = 2.43, Current Epsilon = 0.7397003733882802, Total Step = 30964\n",
            "Current Episode = 148,  Episode Reward = 2.0, Rolling Average = 2.45, Current Epsilon = 0.7323033696543974, Total Step = 31170\n",
            "Current Episode = 149,  Episode Reward = 3.0, Rolling Average = 2.45, Current Epsilon = 0.7323033696543974, Total Step = 31410\n",
            "Current Episode = 150,  Episode Reward = 3.0, Rolling Average = 2.48, Current Epsilon = 0.7323033696543974, Total Step = 31640\n",
            "Current Episode = 151,  Episode Reward = 1.0, Rolling Average = 2.46, Current Epsilon = 0.7323033696543974, Total Step = 31800\n",
            "Current Episode = 152,  Episode Reward = 2.0, Rolling Average = 2.46, Current Epsilon = 0.7249803359578534, Total Step = 32012\n",
            "Current Episode = 153,  Episode Reward = 5.0, Rolling Average = 2.47, Current Epsilon = 0.7249803359578534, Total Step = 32354\n",
            "Current Episode = 154,  Episode Reward = 2.0, Rolling Average = 2.49, Current Epsilon = 0.7249803359578534, Total Step = 32544\n",
            "Current Episode = 155,  Episode Reward = 2.0, Rolling Average = 2.49, Current Epsilon = 0.7249803359578534, Total Step = 32735\n",
            "Current Episode = 156,  Episode Reward = 3.0, Rolling Average = 2.52, Current Epsilon = 0.7249803359578534, Total Step = 32952\n",
            "Current Episode = 157,  Episode Reward = 3.0, Rolling Average = 2.52, Current Epsilon = 0.7177305325982748, Total Step = 33185\n",
            "Current Episode = 158,  Episode Reward = 3.0, Rolling Average = 2.52, Current Epsilon = 0.7177305325982748, Total Step = 33409\n",
            "Current Episode = 159,  Episode Reward = 3.0, Rolling Average = 2.54, Current Epsilon = 0.7177305325982748, Total Step = 33636\n",
            "Current Episode = 160,  Episode Reward = 3.0, Rolling Average = 2.56, Current Epsilon = 0.7177305325982748, Total Step = 33875\n",
            "Current Episode = 161,  Episode Reward = 5.0, Rolling Average = 2.6, Current Epsilon = 0.7105532272722921, Total Step = 34203\n",
            "Current Episode = 162,  Episode Reward = 3.0, Rolling Average = 2.63, Current Epsilon = 0.7105532272722921, Total Step = 34425\n",
            "Current Episode = 163,  Episode Reward = 2.0, Rolling Average = 2.64, Current Epsilon = 0.7105532272722921, Total Step = 34645\n",
            "Current Episode = 164,  Episode Reward = 3.0, Rolling Average = 2.64, Current Epsilon = 0.7105532272722921, Total Step = 34867\n",
            "Current Episode = 165,  Episode Reward = 4.0, Rolling Average = 2.68, Current Epsilon = 0.7034476949995692, Total Step = 35129\n",
            "Current Episode = 166,  Episode Reward = 5.0, Rolling Average = 2.71, Current Epsilon = 0.7034476949995692, Total Step = 35466\n",
            "Current Episode = 167,  Episode Reward = 8.0, Rolling Average = 2.76, Current Epsilon = 0.7034476949995692, Total Step = 35764\n",
            "Current Episode = 168,  Episode Reward = 3.0, Rolling Average = 2.77, Current Epsilon = 0.6964132180495735, Total Step = 36010\n",
            "Current Episode = 169,  Episode Reward = 4.0, Rolling Average = 2.8, Current Epsilon = 0.6964132180495735, Total Step = 36256\n",
            "Current Episode = 170,  Episode Reward = 7.0, Rolling Average = 2.84, Current Epsilon = 0.6964132180495735, Total Step = 36683\n",
            "Current Episode = 171,  Episode Reward = 2.0, Rolling Average = 2.81, Current Epsilon = 0.6964132180495735, Total Step = 36885\n",
            "Current Episode = 172,  Episode Reward = 2.0, Rolling Average = 2.82, Current Epsilon = 0.6894490858690777, Total Step = 37085\n",
            "Current Episode = 173,  Episode Reward = 3.0, Rolling Average = 2.83, Current Epsilon = 0.6894490858690777, Total Step = 37311\n",
            "Current Episode = 174,  Episode Reward = 2.0, Rolling Average = 2.83, Current Epsilon = 0.6894490858690777, Total Step = 37508\n",
            "Current Episode = 175,  Episode Reward = 3.0, Rolling Average = 2.83, Current Epsilon = 0.6894490858690777, Total Step = 37760\n",
            "Current Episode = 176,  Episode Reward = 3.0, Rolling Average = 2.84, Current Epsilon = 0.682554595010387, Total Step = 38008\n",
            "Current Episode = 177,  Episode Reward = 2.0, Rolling Average = 2.85, Current Epsilon = 0.682554595010387, Total Step = 38225\n",
            "Current Episode = 178,  Episode Reward = 5.0, Rolling Average = 2.9, Current Epsilon = 0.682554595010387, Total Step = 38579\n",
            "Current Episode = 179,  Episode Reward = 4.0, Rolling Average = 2.92, Current Epsilon = 0.682554595010387, Total Step = 38857\n",
            "Current Episode = 180,  Episode Reward = 2.0, Rolling Average = 2.91, Current Epsilon = 0.6757290490602831, Total Step = 39045\n",
            "Current Episode = 181,  Episode Reward = 5.0, Rolling Average = 2.94, Current Epsilon = 0.6757290490602831, Total Step = 39379\n",
            "Current Episode = 182,  Episode Reward = 2.0, Rolling Average = 2.96, Current Epsilon = 0.6757290490602831, Total Step = 39592\n",
            "Current Episode = 183,  Episode Reward = 4.0, Rolling Average = 2.98, Current Epsilon = 0.6757290490602831, Total Step = 39880\n",
            "Current Episode = 184,  Episode Reward = 4.0, Rolling Average = 2.99, Current Epsilon = 0.6689717585696803, Total Step = 40150\n",
            "Current Episode = 185,  Episode Reward = 0.0, Rolling Average = 2.99, Current Epsilon = 0.6689717585696803, Total Step = 40277\n",
            "Current Episode = 186,  Episode Reward = 2.0, Rolling Average = 2.98, Current Epsilon = 0.6689717585696803, Total Step = 40497\n",
            "Current Episode = 187,  Episode Reward = 1.0, Rolling Average = 2.98, Current Epsilon = 0.6689717585696803, Total Step = 40675\n",
            "Current Episode = 188,  Episode Reward = 0.0, Rolling Average = 2.97, Current Epsilon = 0.6689717585696803, Total Step = 40808\n",
            "Current Episode = 189,  Episode Reward = 0.0, Rolling Average = 2.93, Current Epsilon = 0.6689717585696803, Total Step = 40944\n",
            "Current Episode = 190,  Episode Reward = 1.0, Rolling Average = 2.93, Current Epsilon = 0.6622820409839835, Total Step = 41120\n",
            "Current Episode = 191,  Episode Reward = 0.0, Rolling Average = 2.9, Current Epsilon = 0.6622820409839835, Total Step = 41259\n",
            "Current Episode = 192,  Episode Reward = 4.0, Rolling Average = 2.92, Current Epsilon = 0.6622820409839835, Total Step = 41557\n",
            "Current Episode = 193,  Episode Reward = 0.0, Rolling Average = 2.89, Current Epsilon = 0.6622820409839835, Total Step = 41690\n",
            "Current Episode = 194,  Episode Reward = 1.0, Rolling Average = 2.88, Current Epsilon = 0.6622820409839835, Total Step = 41871\n",
            "Current Episode = 195,  Episode Reward = 0.0, Rolling Average = 2.83, Current Epsilon = 0.6556592205741436, Total Step = 42004\n",
            "Current Episode = 196,  Episode Reward = 0.0, Rolling Average = 2.8, Current Epsilon = 0.6556592205741436, Total Step = 42148\n",
            "Current Episode = 197,  Episode Reward = 2.0, Rolling Average = 2.8, Current Epsilon = 0.6556592205741436, Total Step = 42353\n",
            "Current Episode = 198,  Episode Reward = 2.0, Rolling Average = 2.8, Current Epsilon = 0.6556592205741436, Total Step = 42572\n",
            "Current Episode = 199,  Episode Reward = 0.0, Rolling Average = 2.78, Current Epsilon = 0.6556592205741436, Total Step = 42715\n",
            "Current Episode = 200,  Episode Reward = 2.0, Rolling Average = 2.78, Current Epsilon = 0.6556592205741436, Total Step = 42927\n",
            "Current Episode = 201,  Episode Reward = 1.0, Rolling Average = 2.77, Current Epsilon = 0.6491026283684022, Total Step = 43115\n",
            "Current Episode = 202,  Episode Reward = 1.0, Rolling Average = 2.76, Current Epsilon = 0.6491026283684022, Total Step = 43316\n",
            "Current Episode = 203,  Episode Reward = 0.0, Rolling Average = 2.71, Current Epsilon = 0.6491026283684022, Total Step = 43457\n",
            "Current Episode = 204,  Episode Reward = 1.0, Rolling Average = 2.68, Current Epsilon = 0.6491026283684022, Total Step = 43621\n",
            "Current Episode = 205,  Episode Reward = 1.0, Rolling Average = 2.65, Current Epsilon = 0.6491026283684022, Total Step = 43816\n",
            "Current Episode = 206,  Episode Reward = 0.0, Rolling Average = 2.64, Current Epsilon = 0.6491026283684022, Total Step = 43943\n",
            "Current Episode = 207,  Episode Reward = 0.0, Rolling Average = 2.62, Current Epsilon = 0.6426116020847181, Total Step = 44094\n",
            "Current Episode = 208,  Episode Reward = 0.0, Rolling Average = 2.59, Current Epsilon = 0.6426116020847181, Total Step = 44243\n",
            "Current Episode = 209,  Episode Reward = 0.0, Rolling Average = 2.57, Current Epsilon = 0.6426116020847181, Total Step = 44369\n",
            "Current Episode = 210,  Episode Reward = 3.0, Rolling Average = 2.57, Current Epsilon = 0.6426116020847181, Total Step = 44611\n",
            "Current Episode = 211,  Episode Reward = 1.0, Rolling Average = 2.56, Current Epsilon = 0.6426116020847181, Total Step = 44782\n",
            "Current Episode = 212,  Episode Reward = 0.0, Rolling Average = 2.54, Current Epsilon = 0.6426116020847181, Total Step = 44927\n",
            "Current Episode = 213,  Episode Reward = 0.0, Rolling Average = 2.47, Current Epsilon = 0.6361854860638709, Total Step = 45050\n",
            "Current Episode = 214,  Episode Reward = 1.0, Rolling Average = 2.46, Current Epsilon = 0.6361854860638709, Total Step = 45220\n",
            "Current Episode = 215,  Episode Reward = 3.0, Rolling Average = 2.47, Current Epsilon = 0.6361854860638709, Total Step = 45503\n",
            "Current Episode = 216,  Episode Reward = 2.0, Rolling Average = 2.47, Current Epsilon = 0.6361854860638709, Total Step = 45715\n",
            "Current Episode = 217,  Episode Reward = 1.0, Rolling Average = 2.46, Current Epsilon = 0.6361854860638709, Total Step = 45880\n",
            "Current Episode = 218,  Episode Reward = 2.0, Rolling Average = 2.45, Current Epsilon = 0.6298236312032323, Total Step = 46067\n",
            "Current Episode = 219,  Episode Reward = 3.0, Rolling Average = 2.47, Current Epsilon = 0.6298236312032323, Total Step = 46306\n",
            "Current Episode = 220,  Episode Reward = 4.0, Rolling Average = 2.49, Current Epsilon = 0.6298236312032323, Total Step = 46559\n",
            "Current Episode = 221,  Episode Reward = 3.0, Rolling Average = 2.49, Current Epsilon = 0.6298236312032323, Total Step = 46809\n",
            "Current Episode = 222,  Episode Reward = 8.0, Rolling Average = 2.56, Current Epsilon = 0.6235253948912, Total Step = 47123\n",
            "Current Episode = 223,  Episode Reward = 0.0, Rolling Average = 2.51, Current Epsilon = 0.6235253948912, Total Step = 47286\n",
            "Current Episode = 224,  Episode Reward = 3.0, Rolling Average = 2.49, Current Epsilon = 0.6235253948912, Total Step = 47528\n",
            "Current Episode = 225,  Episode Reward = 2.0, Rolling Average = 2.49, Current Epsilon = 0.6235253948912, Total Step = 47775\n",
            "Current Episode = 226,  Episode Reward = 9.0, Rolling Average = 2.56, Current Epsilon = 0.617290140942288, Total Step = 48139\n",
            "Current Episode = 227,  Episode Reward = 4.0, Rolling Average = 2.56, Current Epsilon = 0.617290140942288, Total Step = 48435\n",
            "Current Episode = 228,  Episode Reward = 1.0, Rolling Average = 2.55, Current Epsilon = 0.617290140942288, Total Step = 48597\n",
            "Current Episode = 229,  Episode Reward = 5.0, Rolling Average = 2.58, Current Epsilon = 0.617290140942288, Total Step = 48900\n",
            "Current Episode = 230,  Episode Reward = 7.0, Rolling Average = 2.62, Current Epsilon = 0.6111172395328651, Total Step = 49156\n",
            "Current Episode = 231,  Episode Reward = 5.0, Rolling Average = 2.64, Current Epsilon = 0.6111172395328651, Total Step = 49484\n",
            "Current Episode = 232,  Episode Reward = 3.0, Rolling Average = 2.63, Current Epsilon = 0.6111172395328651, Total Step = 49699\n",
            "Current Episode = 233,  Episode Reward = 7.0, Rolling Average = 2.68, Current Epsilon = 0.6111172395328651, Total Step = 49950\n",
            "Current Episode = 234,  Episode Reward = 2.0, Rolling Average = 2.66, Current Epsilon = 0.6050060671375365, Total Step = 50135\n",
            "Current Episode = 235,  Episode Reward = 3.0, Rolling Average = 2.66, Current Epsilon = 0.6050060671375365, Total Step = 50356\n",
            "Current Episode = 236,  Episode Reward = 0.0, Rolling Average = 2.64, Current Epsilon = 0.6050060671375365, Total Step = 50480\n",
            "Current Episode = 237,  Episode Reward = 5.0, Rolling Average = 2.66, Current Epsilon = 0.6050060671375365, Total Step = 50773\n",
            "Current Episode = 238,  Episode Reward = 3.0, Rolling Average = 2.62, Current Epsilon = 0.6050060671375365, Total Step = 50987\n",
            "Current Episode = 239,  Episode Reward = 2.0, Rolling Average = 2.61, Current Epsilon = 0.5989560064661611, Total Step = 51208\n",
            "Current Episode = 240,  Episode Reward = 6.0, Rolling Average = 2.62, Current Epsilon = 0.5989560064661611, Total Step = 51546\n",
            "Current Episode = 241,  Episode Reward = 0.0, Rolling Average = 2.6, Current Epsilon = 0.5989560064661611, Total Step = 51672\n",
            "Current Episode = 242,  Episode Reward = 2.0, Rolling Average = 2.57, Current Epsilon = 0.5989560064661611, Total Step = 51872\n",
            "Current Episode = 243,  Episode Reward = 2.0, Rolling Average = 2.56, Current Epsilon = 0.5929664464014994, Total Step = 52056\n",
            "Current Episode = 244,  Episode Reward = 2.0, Rolling Average = 2.56, Current Epsilon = 0.5929664464014994, Total Step = 52259\n",
            "Current Episode = 245,  Episode Reward = 1.0, Rolling Average = 2.53, Current Epsilon = 0.5929664464014994, Total Step = 52417\n",
            "Current Episode = 246,  Episode Reward = 0.0, Rolling Average = 2.48, Current Epsilon = 0.5929664464014994, Total Step = 52544\n",
            "Current Episode = 247,  Episode Reward = 4.0, Rolling Average = 2.5, Current Epsilon = 0.5929664464014994, Total Step = 52824\n",
            "Current Episode = 248,  Episode Reward = 4.0, Rolling Average = 2.52, Current Epsilon = 0.5870367819374844, Total Step = 53108\n",
            "Current Episode = 249,  Episode Reward = 3.0, Rolling Average = 2.52, Current Epsilon = 0.5870367819374844, Total Step = 53350\n",
            "Current Episode = 250,  Episode Reward = 2.0, Rolling Average = 2.51, Current Epsilon = 0.5870367819374844, Total Step = 53537\n",
            "Current Episode = 251,  Episode Reward = 2.0, Rolling Average = 2.52, Current Epsilon = 0.5870367819374844, Total Step = 53745\n",
            "Current Episode = 252,  Episode Reward = 3.0, Rolling Average = 2.53, Current Epsilon = 0.5870367819374844, Total Step = 53980\n",
            "Current Episode = 253,  Episode Reward = 1.0, Rolling Average = 2.49, Current Epsilon = 0.5811664141181095, Total Step = 54143\n",
            "Current Episode = 254,  Episode Reward = 8.0, Rolling Average = 2.55, Current Epsilon = 0.5811664141181095, Total Step = 54464\n",
            "Current Episode = 255,  Episode Reward = 3.0, Rolling Average = 2.56, Current Epsilon = 0.5811664141181095, Total Step = 54684\n",
            "Current Episode = 256,  Episode Reward = 5.0, Rolling Average = 2.58, Current Epsilon = 0.5811664141181095, Total Step = 54979\n",
            "Current Episode = 257,  Episode Reward = 2.0, Rolling Average = 2.57, Current Epsilon = 0.5753547499769285, Total Step = 55168\n",
            "Current Episode = 258,  Episode Reward = 6.0, Rolling Average = 2.6, Current Epsilon = 0.5753547499769285, Total Step = 55507\n",
            "Current Episode = 259,  Episode Reward = 1.0, Rolling Average = 2.58, Current Epsilon = 0.5753547499769285, Total Step = 55669\n",
            "Current Episode = 260,  Episode Reward = 4.0, Rolling Average = 2.59, Current Epsilon = 0.5753547499769285, Total Step = 55951\n",
            "Current Episode = 261,  Episode Reward = 0.0, Rolling Average = 2.54, Current Epsilon = 0.5696012024771592, Total Step = 56077\n",
            "Current Episode = 262,  Episode Reward = 7.0, Rolling Average = 2.58, Current Epsilon = 0.5696012024771592, Total Step = 56327\n",
            "Current Episode = 263,  Episode Reward = 2.0, Rolling Average = 2.58, Current Epsilon = 0.5696012024771592, Total Step = 56509\n",
            "Current Episode = 264,  Episode Reward = 2.0, Rolling Average = 2.57, Current Epsilon = 0.5696012024771592, Total Step = 56697\n",
            "Current Episode = 265,  Episode Reward = 1.0, Rolling Average = 2.54, Current Epsilon = 0.5696012024771592, Total Step = 56856\n",
            "Current Episode = 266,  Episode Reward = 3.0, Rolling Average = 2.52, Current Epsilon = 0.5639051904523876, Total Step = 57089\n",
            "Current Episode = 267,  Episode Reward = 4.0, Rolling Average = 2.48, Current Epsilon = 0.5639051904523876, Total Step = 57366\n",
            "Current Episode = 268,  Episode Reward = 2.0, Rolling Average = 2.47, Current Epsilon = 0.5639051904523876, Total Step = 57548\n",
            "Current Episode = 269,  Episode Reward = 3.0, Rolling Average = 2.46, Current Epsilon = 0.5639051904523876, Total Step = 57801\n",
            "Current Episode = 270,  Episode Reward = 2.0, Rolling Average = 2.41, Current Epsilon = 0.5582661385478638, Total Step = 58005\n",
            "Current Episode = 271,  Episode Reward = 3.0, Rolling Average = 2.42, Current Epsilon = 0.5582661385478638, Total Step = 58217\n",
            "Current Episode = 272,  Episode Reward = 4.0, Rolling Average = 2.44, Current Epsilon = 0.5582661385478638, Total Step = 58477\n",
            "Current Episode = 273,  Episode Reward = 1.0, Rolling Average = 2.42, Current Epsilon = 0.5582661385478638, Total Step = 58631\n",
            "Current Episode = 274,  Episode Reward = 3.0, Rolling Average = 2.43, Current Epsilon = 0.5582661385478638, Total Step = 58869\n",
            "Current Episode = 275,  Episode Reward = 8.0, Rolling Average = 2.48, Current Epsilon = 0.5526834771623851, Total Step = 59166\n",
            "Current Episode = 276,  Episode Reward = 8.0, Rolling Average = 2.53, Current Epsilon = 0.5526834771623851, Total Step = 59484\n",
            "Current Episode = 277,  Episode Reward = 4.0, Rolling Average = 2.55, Current Epsilon = 0.5526834771623851, Total Step = 59766\n",
            "Current Episode = 278,  Episode Reward = 3.0, Rolling Average = 2.53, Current Epsilon = 0.5471566423907612, Total Step = 60026\n",
            "Current Episode = 279,  Episode Reward = 6.0, Rolling Average = 2.55, Current Epsilon = 0.5471566423907612, Total Step = 60375\n",
            "Current Episode = 280,  Episode Reward = 11.0, Rolling Average = 2.64, Current Epsilon = 0.5471566423907612, Total Step = 60807\n",
            "Current Episode = 281,  Episode Reward = 5.0, Rolling Average = 2.64, Current Epsilon = 0.5416850759668536, Total Step = 61130\n",
            "Current Episode = 282,  Episode Reward = 2.0, Rolling Average = 2.64, Current Epsilon = 0.5416850759668536, Total Step = 61324\n",
            "Current Episode = 283,  Episode Reward = 1.0, Rolling Average = 2.61, Current Epsilon = 0.5416850759668536, Total Step = 61497\n",
            "Current Episode = 284,  Episode Reward = 4.0, Rolling Average = 2.61, Current Epsilon = 0.5416850759668536, Total Step = 61774\n",
            "Current Episode = 285,  Episode Reward = 5.0, Rolling Average = 2.66, Current Epsilon = 0.536268225207185, Total Step = 62111\n",
            "Current Episode = 286,  Episode Reward = 3.0, Rolling Average = 2.67, Current Epsilon = 0.536268225207185, Total Step = 62345\n",
            "Current Episode = 287,  Episode Reward = 3.0, Rolling Average = 2.69, Current Epsilon = 0.536268225207185, Total Step = 62606\n",
            "Current Episode = 288,  Episode Reward = 2.0, Rolling Average = 2.71, Current Epsilon = 0.536268225207185, Total Step = 62790\n",
            "Current Episode = 289,  Episode Reward = 5.0, Rolling Average = 2.76, Current Epsilon = 0.5309055429551132, Total Step = 63122\n",
            "Current Episode = 290,  Episode Reward = 3.0, Rolling Average = 2.78, Current Epsilon = 0.5309055429551132, Total Step = 63361\n",
            "Current Episode = 291,  Episode Reward = 2.0, Rolling Average = 2.8, Current Epsilon = 0.5309055429551132, Total Step = 63562\n",
            "Current Episode = 292,  Episode Reward = 3.0, Rolling Average = 2.79, Current Epsilon = 0.5309055429551132, Total Step = 63782\n",
            "Current Episode = 293,  Episode Reward = 4.0, Rolling Average = 2.83, Current Epsilon = 0.525596487525562, Total Step = 64070\n",
            "Current Episode = 294,  Episode Reward = 4.0, Rolling Average = 2.86, Current Epsilon = 0.525596487525562, Total Step = 64330\n",
            "Current Episode = 295,  Episode Reward = 4.0, Rolling Average = 2.9, Current Epsilon = 0.525596487525562, Total Step = 64596\n",
            "Current Episode = 296,  Episode Reward = 2.0, Rolling Average = 2.92, Current Epsilon = 0.525596487525562, Total Step = 64777\n",
            "Current Episode = 297,  Episode Reward = 3.0, Rolling Average = 2.93, Current Epsilon = 0.5203405226503064, Total Step = 65026\n",
            "Current Episode = 298,  Episode Reward = 5.0, Rolling Average = 2.96, Current Epsilon = 0.5203405226503064, Total Step = 65348\n",
            "Current Episode = 299,  Episode Reward = 7.0, Rolling Average = 3.03, Current Epsilon = 0.5203405226503064, Total Step = 65756\n",
            "Current Episode = 300,  Episode Reward = 4.0, Rolling Average = 3.05, Current Epsilon = 0.5151371174238033, Total Step = 66022\n",
            "Current Episode = 301,  Episode Reward = 4.0, Rolling Average = 3.08, Current Epsilon = 0.5151371174238033, Total Step = 66271\n",
            "Current Episode = 302,  Episode Reward = 3.0, Rolling Average = 3.1, Current Epsilon = 0.5151371174238033, Total Step = 66515\n",
            "Current Episode = 303,  Episode Reward = 3.0, Rolling Average = 3.13, Current Epsilon = 0.5151371174238033, Total Step = 66761\n",
            "Current Episode = 304,  Episode Reward = 13.0, Rolling Average = 3.25, Current Epsilon = 0.5099857462495653, Total Step = 67265\n",
            "Current Episode = 305,  Episode Reward = 3.0, Rolling Average = 3.27, Current Epsilon = 0.5099857462495653, Total Step = 67493\n",
            "Current Episode = 306,  Episode Reward = 1.0, Rolling Average = 3.28, Current Epsilon = 0.5099857462495653, Total Step = 67662\n",
            "Current Episode = 307,  Episode Reward = 12.0, Rolling Average = 3.4, Current Epsilon = 0.5048858887870696, Total Step = 68107\n",
            "Current Episode = 308,  Episode Reward = 3.0, Rolling Average = 3.43, Current Epsilon = 0.5048858887870696, Total Step = 68358\n",
            "Current Episode = 309,  Episode Reward = 4.0, Rolling Average = 3.47, Current Epsilon = 0.5048858887870696, Total Step = 68657\n",
            "Current Episode = 310,  Episode Reward = 4.0, Rolling Average = 3.48, Current Epsilon = 0.5048858887870696, Total Step = 68941\n",
            "Current Episode = 311,  Episode Reward = 1.0, Rolling Average = 3.48, Current Epsilon = 0.4998370298991989, Total Step = 69111\n",
            "Current Episode = 312,  Episode Reward = 6.0, Rolling Average = 3.54, Current Epsilon = 0.4998370298991989, Total Step = 69458\n",
            "Current Episode = 313,  Episode Reward = 5.0, Rolling Average = 3.59, Current Epsilon = 0.4998370298991989, Total Step = 69794\n",
            "Current Episode = 314,  Episode Reward = 1.0, Rolling Average = 3.59, Current Epsilon = 0.4998370298991989, Total Step = 69962\n",
            "Current Episode = 315,  Episode Reward = 4.0, Rolling Average = 3.6, Current Epsilon = 0.49483865960020695, Total Step = 70247\n",
            "Current Episode = 316,  Episode Reward = 6.0, Rolling Average = 3.64, Current Epsilon = 0.49483865960020695, Total Step = 70607\n",
            "Current Episode = 317,  Episode Reward = 4.0, Rolling Average = 3.67, Current Epsilon = 0.49483865960020695, Total Step = 70868\n",
            "Current Episode = 318,  Episode Reward = 1.0, Rolling Average = 3.66, Current Epsilon = 0.4898902730042049, Total Step = 71025\n",
            "Current Episode = 319,  Episode Reward = 3.0, Rolling Average = 3.66, Current Epsilon = 0.4898902730042049, Total Step = 71242\n",
            "Current Episode = 320,  Episode Reward = 1.0, Rolling Average = 3.63, Current Epsilon = 0.4898902730042049, Total Step = 71400\n",
            "Current Episode = 321,  Episode Reward = 5.0, Rolling Average = 3.65, Current Epsilon = 0.4898902730042049, Total Step = 71692\n",
            "Current Episode = 322,  Episode Reward = 4.0, Rolling Average = 3.61, Current Epsilon = 0.4898902730042049, Total Step = 71970\n",
            "Current Episode = 323,  Episode Reward = 1.0, Rolling Average = 3.62, Current Epsilon = 0.48499137027416284, Total Step = 72135\n",
            "Current Episode = 324,  Episode Reward = 7.0, Rolling Average = 3.66, Current Epsilon = 0.48499137027416284, Total Step = 72387\n",
            "Current Episode = 325,  Episode Reward = 7.0, Rolling Average = 3.71, Current Epsilon = 0.48499137027416284, Total Step = 72784\n",
            "Current Episode = 326,  Episode Reward = 4.0, Rolling Average = 3.66, Current Epsilon = 0.4801414565714212, Total Step = 73073\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-a84f97098e58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#Now we update the weights of the agent.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mepisode_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mMax_Qs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmax_q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-678bc81617ca>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m#Feed replays to Neural nets to observe expected q_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "agent = DDDQNAgent(env)\n",
        "rolling_100 = deque(maxlen=100)\n",
        "total_iter = 1\n",
        "\n",
        "for episode in range(100000): #Loops for passed range number of total episodes.\n",
        "\n",
        "    #Grab the first frame, and then jam 4 of them together rather unceremoniously. Standard practice for Atari Games. Helps to demonstrate velocity.\n",
        "    state = env.reset()\n",
        "    state = agent.preProcess(state)\n",
        "    state = np.stack((state, state, state, state))\n",
        "\n",
        "    #Declaring in per-episode totals\n",
        "    Max_Qs = 0 \n",
        "    episode_reward = 0\n",
        "    episode_loss = 0\n",
        "\n",
        "    for iter in range(100000): #Very ambitious limit to number of steps in a given episode. Will be very surprised if we get anywhere near this.\n",
        "\n",
        "        #Classic Sarsa style step, take action and observe output from environment. Then store those results for experience recall\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, dead, _ = env.step(action)\n",
        "        next_state = agent.preProcess(next_state)\n",
        "        next_state = np.stack((next_state, state[0], state[1], state[2]))\n",
        "        agent.storeResults(state, action, reward, next_state, dead)\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "\n",
        "        #Now we update the weights of the agent.\n",
        "        loss, max_q = agent.train()\n",
        "        episode_loss += loss\n",
        "        Max_Qs += max_q\n",
        "\n",
        "        total_iter += 1\n",
        "\n",
        "        #Update the epsilon as we go to improve late training performance. Exploration/Exploitation trade off.\n",
        "        if total_iter % 1000 == 0:\n",
        "            agent.updateEpsilon()\n",
        "\n",
        "        if dead:\n",
        "            #Episode has ended. DDDQNAgent has run out of lives.\n",
        "\n",
        "            #Get episode statistics ready.\n",
        "            rolling_100.append(episode_reward)\n",
        "            avg_max_q_val = Max_Qs / iter\n",
        "\n",
        "            #Log for monitoring\n",
        "            print(f\"Current Episode = {episode},  Episode Reward = {episode_reward}, Rolling Average = {np.mean(rolling_100)}, Current Epsilon = {agent.epsilon}, Total Step = {total_iter}\")\n",
        "\n",
        "            #Send to WandB for remote monitoring and summary graphing.\n",
        "            wandb.log({'Episode': episode, \\\n",
        "                           \"Average reward\": np.mean(rolling_100), \\\n",
        "                           \"Total Loss\": episode_loss, \\\n",
        "                           \"Average maxQ\": round(avg_max_q_val, 2), \\\n",
        "                           \"total_step\": total_iter})\n",
        "            \n",
        "            #Save model as pkl to store progress / use later to play.\n",
        "            if episode % 20 == 0:\n",
        "                file_name = \"./DQNBreakOut-\" + str(episode) + \"-\" + str(round(np.mean(rolling_100))) + '.pkl'\n",
        "                torch.save(agent.main_model.state_dict(), file_name)\n",
        "            \n",
        "            #Make sure target and online model weights match\n",
        "            agent.target_model.load_state_dict(agent.main_model.state_dict())\n",
        "\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-AQNniGkltD"
      },
      "source": [
        "Acknowledgements:\n",
        "\n",
        "  Playing Atari with Deep Learning - Deepmind - Available at:  https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
        "\n",
        "  Official Pytorch DQN tutorial - Available at: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "  \n",
        "  Neural Network and Hyperparameters informed by:\n",
        "  https://lzzmm.github.io/2021/11/05/breakout/\n",
        "\n",
        "  Adaptation of model to breakout learned from:\n",
        "\n",
        "  https://github.com/AdrianHsu/breakout-Deep-Q-Network\n",
        "\n",
        "  https://keras.io/examples/rl/deep_q_network_breakout/\n",
        "\n",
        "  https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26\n",
        "\n",
        "  Use of pytorch in this environment informed by:\n",
        "\n",
        "  https://www.mlq.ai/deep-reinforcement-learning-pytorch-implementation/\n",
        "\n",
        "  https://github.com/iKintosh/DQN-breakout-Pytorch\n",
        "\n",
        "  https://github.com/bhctsntrk/OpenAIPong-DQN\n",
        "\n",
        "  https://github.com/jasonbian97/Deep-Q-Learning-Atari-Pytorch\n",
        "\n",
        "\n",
        "  Prioritised replay researched from (Not able to implement):\n",
        "\n",
        "  https://github.com/sfyzsr/Reinforcement-Learning-for-Atari\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dctlNQGvchO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "BreakoutDDDQN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}